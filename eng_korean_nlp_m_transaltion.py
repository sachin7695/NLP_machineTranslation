# -*- coding: utf-8 -*-
"""Eng_Korean_NLP_M_Transaltion.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10xjS8hrHiDSNmT5Z0xJu-tUeJxz0CpU1
"""

from google.colab import drive
drive.mount('/content/drive/')

import string
import re
import pandas as pd
from numpy import array, argmax, random, take

"""# Data Cleaning

"""

# loading the dataset of english training set
data_path = "/content/ted_train_en-ko.raw.en"
with open(data_path, 'r', encoding = 'utf-8') as f:
  lines = f.read()

lines

# splitting the texts into a newline
def to_lines(text):
  sents = text.strip().split('\n')
  sents = [i.split('\t') for i in sents]
  return sents

en_ko = to_lines(lines)
en_ko[:5]

type(en_ko)

en_ko = array(en_ko)
en_ko[:5]

en_ko.shape

en_ko = en_ko[:10000]

# remove punctuation
en_ko[:, 0] = [s.translate(str.maketrans('','',string.punctuation)) for s in en_ko[:, 0]]
en_ko[:5]

# make lower case
for i in range(len(en_ko)):
  en_ko[i,0] = en_ko[i, 0].lower()
en_ko

type(en_ko)

"""type(en_ko)"""

en_ko.tolist

en_ko

en_ko_list = list(en_ko)
type(en_ko_list)

import nltk
nltk.download('stopwords')
import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
flat_list = []
for sublist in en_ko_list:
    for item in sublist:
      item = item.split()
      temp = " ".join(item)

      # Remove stop words
      stop_words = set(stopwords.words('english'))
      word_tokens = word_tokenize(temp)
      filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]
      filtered_sentence = []


      for w in word_tokens:
        if w not in stop_words:
          filtered_sentence.append(w)
      line = filtered_sentence
      flat_list.append(" ".join(line))

flat_list[:5]

# loading dataset of korean training set
data_path_1 = "/content/ted_train_en-ko.raw.ko"
with open(data_path_1, 'r', encoding = 'utf-8') as f:
  lines_1 = f.read()
print("lines readed")

ko_en = to_lines(lines_1)
ko_en[:5]

ko_en = array(ko_en)
ko_en[:5]

ko_en = ko_en[:10000]

ko_en[:, 0] = [s.translate(str.maketrans('','',string.punctuation)) for s in ko_en[:, 0]]
# remove punctuation
ko_en[:5]

# make lower case
for i in range(len(ko_en)):
  ko_en[i,0] = ko_en[i, 0].lower()
ko_en

ko_en_list = list(ko_en)
type(en_ko_list)

import nltk
nltk.download('stopwords')
import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
flat_list_1 = []
for sublist_1 in ko_en_list:
    for item in sublist_1:
      item = item.split()
      temp = " ".join(item)

      # Remove stop words
      stop_words = set(stopwords.words('finnish'))
      word_tokens = word_tokenize(temp)
      filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]
      filtered_sentence = []


      for w in word_tokens:
        if w not in stop_words:
          filtered_sentence.append(w)
      line = filtered_sentence
      flat_list_1.append(" ".join(line))

flat_list_1[:5]

"""#Files
Each line in english contains an English sentence with the respective translation in each line of korean. View the first two lines from each file.
"""

for sample_i in range(5):
    print('English sample {}:  {}'.format(sample_i + 1, flat_list[:5][sample_i]))
    print('korean sample {}:  {}\n'.format(sample_i + 1, flat_list_1[:5][sample_i]))

"""#Vocabulary

"""

import collections
english_words_counter = collections.Counter([word for sentence in flat_list[:] for word in sentence.split()])
korean_words_counter = collections.Counter([word for sentence in flat_list_1[:] for word in sentence.split()])

print('{} English words.'.format(len([word for sentence in flat_list[:] for word in sentence.split()])))
print('{} unique English words.'.format(len(english_words_counter)))
print('10 Most common words in the English dataset:')
print('"' + '" "'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '"')
print()
print('{} korean words.'.format(len([word for sentence in flat_list_1[:] for word in sentence.split()])))
print('{} unique korean words.'.format(len(korean_words_counter)))
print('10 Most common words in the korean dataset:')
print('"' + '" "'.join(list(zip(*korean_words_counter.most_common(10)))[0]) + '"')

"""#Preprocess
For this project, you won't use text data as input to your model. Instead, you'll convert the text into sequences of integers using the following preprocess methods:

1.Tokenize the words into ids
2.Add padding to make all the sequences the same length.
"""

!pip install keras==2.0.6
!pip install tensorflow==1.13.2
import sys
sys.path.insert(0,'/content/drive/MyDrive/Colab Notebooks')
import project_tests as tests
from keras.preprocessing.text import Tokenizer
def tokenize(x):
    """
    Tokenize x
    :param x: List of sentences/strings to be tokenized
    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)
    """
    # TODO: Implement
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(x)
    return tokenizer.texts_to_sequences(x), tokenizer

tests.test_tokenize(tokenize)

# Tokenize Example output
text_sentences = [
    'The quick brown fox jumps over the lazy dog .',
    'By Jove , my quick study of lexicography won a prize .',
    'This is a short sentence .']
text_tokenized, text_tokenizer = tokenize(text_sentences)
print(text_tokenizer.word_index)
print()
for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):
    print('Sequence {} in x'.format(sample_i + 1))
    print('  Input:  {}'.format(sent))
    print('  Output: {}'.format(token_sent))

# padding implementation
from keras.preprocessing.sequence import pad_sequences
import numpy as np
def pad(x, length=None):
    """
    Pad x
    :param x: List of sequences.
    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.
    :return: Padded numpy array of sequences
    """
    # TODO: Implement
    return pad_sequences(x, maxlen=length, padding='post')

tests.test_pad(pad)

# Pad Tokenized output
test_pad = pad(text_tokenized)
for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):
    print('Sequence {} in x'.format(sample_i + 1))
    print('  Input:  {}'.format(np.array(token_sent)))
    print('  Output: {}'.format(pad_sent))

# preprocess pipeline
def preprocess(x, y):
    """
    Preprocess x and y
    :param x: Feature List of sentences
    :param y: Label List of sentences
    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)
    """
    preprocess_x, x_tk = tokenize(x)
    preprocess_y, y_tk = tokenize(y)

    preprocess_x = pad(preprocess_x)
    preprocess_y = pad(preprocess_y)

    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions
    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)

    return preprocess_x, preprocess_y, x_tk, y_tk

preproc_english_sentences, preproc_korean_sentences, english_tokenizer, korean_tokenizer =\
    preprocess(flat_list[:], flat_list_1[:])
    
max_english_sequence_length = preproc_english_sentences.shape[1]
max_korean_sequence_length = preproc_korean_sentences.shape[1]
english_vocab_size = len(english_tokenizer.word_index)+1
korean_vocab_size = len(korean_tokenizer.word_index)+1

print('Data Preprocessed')
print("Max English sentence length:", max_english_sequence_length)
print("Max korean sentence length:", max_korean_sequence_length)
print("English vocabulary size:", english_vocab_size)
print("korean vocabulary size:", korean_vocab_size)

"""#Ids Back to Text
The neural network will be translating the input to words ids, which isn't the final form we want. We want the Korean translation. The function logits_to_text will bridge the gab between the logits from the neural network to the French translation. You'll be using this function to better understand the output of the neural network.
"""

# Ids back to text
def logits_to_text(logits, tokenizer):
    """
    Turn logits from a neural network into text using the tokenizer
    :param logits: Logits from a neural network
    :param tokenizer: Keras Tokenizer fit on the labels
    :return: String that represents the text of the logits
    """
    index_to_words = {id: word for word, id in tokenizer.word_index.items()}
    index_to_words[0] = '<PAD>'

    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])

print('`logits_to_text` function loaded.')

"""#Model Implementation"""

from keras.models import Model, Sequential
from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM
from keras.layers.embeddings import Embedding
from keras.optimizers import Adam 
from keras.losses import sparse_categorical_crossentropy

import tensorflow

def simple_model(input_shape, output_sequence_length, english_vocab_size, korean_vocab_size):
    """
    Build and train a basic RNN on x and y
    :param input_shape: Tuple of input shape
    :param output_sequence_length: Length of output sequence
    :param english_vocab_size: Number of unique English words in the dataset
    :param korean_vocab_size: Number of unique koreanb words in the dataset
    :return: Keras model built, but not trained
    """
    # Hyperparameters
    learning_rate = 0.005
    
    # TODO: Build the layers
    model = Sequential()
    model.add(GRU(64, input_shape=input_shape[1:], return_sequences=True))
    model.add(TimeDistributed(Dense(128, activation='relu')))
    model.add(Dropout(0.5))
    model.add(TimeDistributed(Dense(korean_vocab_size, activation='softmax'))) 

    # Compile model
    model.compile(loss=sparse_categorical_crossentropy,
                  optimizer=Adam(learning_rate),
                  metrics=['accuracy'])
    return model

tests.test_simple_model(simple_model)

# Reshaping the input to work with a basic RNN
tmp_x = pad(preproc_english_sentences, max_korean_sequence_length)
tmp_x = tmp_x.reshape((-1, preproc_korean_sentences.shape[-2], 1))

# Train the neural network
simple_rnn_model = simple_model(
    tmp_x.shape,
    max_korean_sequence_length,
    english_vocab_size,
    korean_vocab_size)

print(simple_rnn_model.summary())

with tensorflow.device('/GPU:0'):
  simple_rnn_model.fit(tmp_x, preproc_korean_sentences, batch_size=16, epochs=5, validation_split=0.2)

# Print prediction(s)
print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], korean_tokenizer))
print(tmp_x.shape)

# Print prediction(s)
print("Prediction:")
print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], korean_tokenizer))

print("\nCorrect Translation:")
print(flat_list_1[:1])

print("\nOriginal text:")
print(flat_list[:1])

simple_rnn_model.save("/content/model_weights")
#saving model weights for future access

